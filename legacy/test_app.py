import streamlit as st
import os
import io
import platform 
import asyncio
import nest_asyncio
from langchain_openai import ChatOpenAI
from langchain_anthropic import ChatAnthropic

from langchain_core.prompts import (
    SystemMessagePromptTemplate,
    HumanMessagePromptTemplate,
    ChatPromptTemplate,
    MessagesPlaceholder
)

from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_community.chat_message_histories import SQLChatMessageHistory

from langchain_core.output_parsers import StrOutputParser
from langchain_core.messages import HumanMessage, AIMessage

from langchain_community.vectorstores import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
from langgraph.checkpoint.memory import MemorySaver

from tools.rfp_extracter import extract_text_from_pdf, extract_text_from_docx, convert_rfp_for_RAG

# í™˜ê²½ ë³€ìˆ˜ ì„¸íŒ… (í•„ìš”ì‹œ)
from dotenv import load_dotenv
load_dotenv()

# CSS
st.markdown(
    """
    <style>
    .title {
        color: white;
        font-size: 4em;
        text-align: center;
        display: flex;
        justify-content: center;
        align-items: center;
    }
    body, .stApp, .block-container, .main, footer {
        background: #23272F !important;
    }
    /* ì…ë ¥ì°½ í¼ ìì²´ë¥¼ ë” ë„“ê²Œ */
    .stChatInputContainer form {
        width: 100vw !important;   /* ì›í•˜ëŠ” í­(%)ë¡œ ì¡°ì •, 80vwë„ ê°€ëŠ¥ */
        max-width: 1200px !important;
        min-width: 360px !important;
        margin: 0 auto !important;
    }
    /* í…ìŠ¤íŠ¸ ë°•ìŠ¤(ì‹¤ì œ ì…ë ¥ì°½)ë„ í¬ê³  ì„ ëª…í•˜ê²Œ */
    .stChatInputContainer textarea, .stChatInputContainer input {
        width: 100% !important;
        min-height: 56px !important;
        height: 56px !important;
        font-size: 1.2em !important;
        padding: 17px 22px !important;
        border-radius: 12px !important;
        background: #23272F !important;
        color: #F3F6FA !important;
        border: 2px solid #3AAED8 !important;
        box-sizing: border-box;
    }
    /* ë¹„í™œì„±í™” ë²„íŠ¼ë„ í†¤ ë§ì¶”ê¸° */
    .stButton>button:disabled, .stButton>button[disabled] {
        background: #384150 !important;
        color: #bfc6d2 !important;
        opacity: 0.70 !important;
    }
    /* ì—…ë¡œë“œ ì•ˆë‚´ ë©”ì‹œì§€ ë°ê²Œ */
    .stSidebar .stAlert, .stFileUploader .css-1p89gle, .stFileUploader .css-115gedg, .stFileUploader label, .stFileUploader span {
        color: #aee6ff !important;
    }
    </style>
    """,
    unsafe_allow_html=True
)

# ë©”ì¸ í˜ì´ì§€
st.markdown("<h1 style='color: #ffffff;'>ğŸ–¥ï¸ KBN ì´íŒ ì œí’ˆ ì¶”ì²œ ì±—ë´‡</h1>", unsafe_allow_html=True)
st.markdown("<span style='color: #e0e0e0; font-size: 18px;'>âœ¨ Ask questions to the GPT-4o agent</span>", unsafe_allow_html=True)

# ===== ì‚¬ì´ë“œë°”ì— ë²„íŠ¼ & íŒŒì¼ ì—…ë¡œë“œ =====
with st.sidebar:
    if st.button("ëŒ€í™” ì´ˆê¸°í™”", key="init_chat"):
        st.session_state.chat_history = []
        st.session_state["rag_term"] = ""
        if "uploaded_file_path" in st.session_state and st.session_state["uploaded_file_path"]:
            try:
                os.remove(st.session_state["uploaded_file_path"])
                st.success(f"ì„ì‹œ íŒŒì¼ ì‚­ì œ ì™„ë£Œ: `{os.path.basename(st.session_state['uploaded_file_path'])}`")
            except Exception as e:
                st.warning(f"ì„ì‹œ íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {e}")
            st.session_state["uploaded_file_path"] = None

    st.subheader("ğŸ“„ íŒŒì¼ ì—…ë¡œë“œ")
    uploaded_file = st.file_uploader(
        "PDF ë˜ëŠ” DOCX íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”",
        type=["pdf", "docx"],
        key="file_uploader"
    )

    if uploaded_file is not None:
        # íŒŒì¼ ì„ì‹œ ì €ì¥
        file_bytes = uploaded_file.read()
        tmp_path = os.path.join("/tmp", uploaded_file.name)
        with open(tmp_path, "wb") as f:
            f.write(file_bytes)
        st.session_state["uploaded_file_path"] = tmp_path
        st.success(f"ì—…ë¡œë“œ ì™„ë£Œ: `{uploaded_file.name}`")

        file_type = None
        if uploaded_file.name.lower().endswith(".pdf"):
            file_type = "pdf"
        elif uploaded_file.name.lower().endswith(".docx"):
            file_type = "docx"

        # íŒŒì¼ buffer
        file_buffer = io.BytesIO(file_bytes)
        file_buffer.seek(0)

        try:
            if file_type == "pdf":
                st.info("PDF íŒŒì¼ë¡œ ì¸ì‹ë¨. í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤...")
                txt_path = extract_text_from_pdf(file_buffer)      # ì„ì‹œ txt ê²½ë¡œ(str)
            elif file_type == "docx":
                st.info("DOCX íŒŒì¼ë¡œ ì¸ì‹ë¨. í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤...")
                txt_path = extract_text_from_docx(file_buffer)     # ì„ì‹œ txt ê²½ë¡œ(str)
            else:
                st.warning("ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤.")
                txt_path = None

            if txt_path:
                st.success("í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ!")
                rag_term = convert_rfp_for_RAG(txt_path)
                st.session_state["rag_term"] = rag_term
                st.markdown("#### â¡ï¸ ì¶”ì¶œëœ RFP ìš”ì•½")
                st.text_area("->", rag_term, height=300, disabled=True)
        except Exception as e:
            st.error(f"{file_type.upper()} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
    else:
        st.session_state["uploaded_file_path"] = None
        st.info("PDF ë˜ëŠ” DOCX íŒŒì¼ì„ ì„ íƒí•´ ì£¼ì„¸ìš”.")
        st.session_state["rag_term"] = ""

# ===== Windows í™˜ê²½ ë¹„ë™ê¸° ì„¤ì • =====
if platform.system() == "Windows":
    asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())
nest_asyncio.apply()

if "event_loop" not in st.session_state:
    loop = asyncio.new_event_loop()
    st.session_state.event_loop = loop
    asyncio.set_event_loop(loop)

# ëŒ€í™” ì„¸ì…˜ ì´ˆê¸°í™”
if "history" not in st.session_state:
    st.session_state.history = []

MODEL_NAME = "gpt-4o"
OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY")

llm = ChatOpenAI(
    model = MODEL_NAME,
    temperature = 0.2,
    max_tokens = 16384,
    api_key= OPENAI_API_KEY,
)

# ëŒ€í™” ê¸°ë¡ ì¶œë ¥ í•¨ìˆ˜
def print_message():
    for message in st.session_state.history:
        if message["role"] == "user":
            st.chat_message("user", avatar="ğŸ§‘â€ğŸ’»").markdown(message["content"])
        elif message["role"] == "assistant":
            # í…ìŠ¤íŠ¸ë¥¼ span íƒœê·¸ë¡œ ê°ì‹¸ì„œ ìƒ‰ìƒ ì§€ì • (HTML ì§€ì›)
            assistant_text = f"<span style='color:#fff'>{message['content']}</span>"
            st.chat_message("assistant", avatar="ğŸ¤–").markdown(assistant_text, unsafe_allow_html=True)


print_message()

# ì§ˆë¬¸ ì²˜ë¦¬ í•¨ìˆ˜ (ë¹„ë™ê¸°)
async def process_query(query):
    try:
        # PDF í…ìŠ¤íŠ¸ ì¶”ì¶œëœ í´ë”ê°€ ìˆìœ¼ë©´ ë‚´ìš© ê²°í•©í•˜ì—¬ contextë¡œ ì‚¬ìš©
        context = ""
        if "preprocessed_pdf_dir" in st.session_state:
            output_dir = st.session_state["preprocessed_pdf_dir"]
            txt_files = sorted(f for f in os.listdir(output_dir) if f.endswith(".txt"))
            for file in txt_files:
                with open(os.path.join(output_dir, file), encoding="utf-8") as f:
                    context += f.read() + "\n"
        else:
            context = ""

        prompt = f"""You are a helpful assistant. Answer the user's question based on the following context (if any).
        If there is no context, just answer as best as you can.
        ---
        Context:
        {context}
        ---
        Question: {query}
        Answer:"""

        resp = await llm.ainvoke([HumanMessage(content=prompt)])
        return resp.content
    except Exception as e:
        return f"âŒ Error: {e}"

# ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
user_query = st.chat_input("ğŸ’¬ Enter your question")
if user_query:
    st.chat_message("user", avatar="ğŸ§‘â€ğŸ’»").markdown(user_query)
    with st.chat_message("assistant", avatar="ğŸ¤–"):
        with st.spinner("Generating answer..."):
            answer = st.session_state.event_loop.run_until_complete(process_query(user_query))
            st.markdown(answer)
    st.session_state.history.append({"role": "user", "content": user_query})
    st.session_state.history.append({"role": "assistant", "content": answer})
    st.rerun()

# ë¦¬ì…‹ ë²„íŠ¼(ì‚¬ì´ë“œë°”)
with st.sidebar:
    st.subheader("Reset Button")
    if st.button("Reset Conversation", use_container_width=True):
        st.session_state.history = []
        st.success("âœ… Conversation has been reset.")
        st.rerun()